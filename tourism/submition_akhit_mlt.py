# -*- coding: utf-8 -*-
"""Submition akhit MLT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZsaiVkQGIOLKIvT4xetfnP38oBpokd_N

#Sistem Rekomendasi mencari lokasi wisata di Indonesia

Sistem digunakan untuk meberikan rekomendasi nama destinasi lokasi wisata berdasarkan cotent bases filtering (yang digunakan adalah nama lokasi yang di filter berdasarkan kesamaan lokasi/kota)

## Data Source
Data diambil dari kaggle, terdapat 4 file di dalam file zip

[Source data Indonesia Tourism Destination](https://www.kaggle.com/aprabowo/indonesia-tourism-destination)

Import semua Library yang akan digunakan, import dilakukan pada proses awal karena untuk mempermudah proses pengerjaan.
"""

!pip install kaggle

import numpy as np
import pandas as pd
import seaborn as sns
import tensorflow as tf
import matplotlib.pyplot as plt

from pathlib import Path
from google.colab import files
from tensorflow.keras import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Embedding
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer

"""Karena Data diambil dari kaggle, untuk mempermudah proses load data bisa menggunakan library kaggle.

Install kaggle, lalu upload file key token yang telah di *generate*  dari account yang dimiliki. Lalu membuat direktory baru bernama kaggle, pindah data kaggle.json yang sudah kita upload, arahkan ke folder kaggle yang sudah dibuat sebelumnya, lalu atur *permission access* supaya owner/user, grup, other bisa untuk *read, write and execution* terhadap data kaggle.json dengan menggunakan ```chmood 600```
"""

upload = files.upload()
for k in upload.keys():
  print(k)

!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json

"""## Data Loading

Download dataset Indonesia torism destination dengan menggunakan API kaggle file akan disimpan dalam ```/content/```
"""

!kaggle datasets download -d aprabowo/indonesia-tourism-destination

"""### Unzip File zip

Setelah data di download, ternyata data berbentuk file zip, yang didalamnya terdapat 4 file csv yaitu:
1. tourism_ with_id.csv : file ini berisi informasi lokasi wisata dari 5 kota di Indonesia 
2. user.csv : file ini berisi mengenai data pengguna (data dummy)
3. tourism_rating.csv : file ini berisi mengenai data rating dari bebrapa tempat lokasi wissata yang diberikan oleh user 
4. package_tourism.csv : file ini berisi package lokasi wisata

Buat satu directory yang berfungsi untuk menampung data dari indonesia-tourism-destination.zip dengan menggunakan 

```!mkdir name_directory```

untuk ektract data zip bisa menggunakan script 



```
!unzip -q indonesia-tourism-destination.zip
```
kenapa terdapat -q didalam script ```!unzip``` dikarenakan untuk menyembunyikan comment yang muncul
"""

!mkdir -p /content/destination_tourism
!unzip -q indonesia-tourism-destination.zip -d /content/destination_tourism

"""### Load Data
Load data masing-masing file kedalam variable yang mengambarkan nama dari file tersebut, data di load dengan menggunakan library **Pandas**

Kemudian cek berapa jumlah masing-masing data dalam varible tersebut dengan menggunkan fungsi ```len()```
"""

package_tourism = pd.read_csv('/content/destination_tourism/package_tourism.csv')
tourism_rating = pd.read_csv('/content/destination_tourism/tourism_rating.csv')
tourism_user = pd.read_csv('/content/destination_tourism/user.csv')
tourism_info = pd.read_csv('/content/destination_tourism/tourism_with_id.csv')

print(f'Jumlah data package tourism : {len(package_tourism)}')
print(f'Jumlah data tourism rating: {len(tourism_rating)}')
print(f'Jumlah user: {len(tourism_user)}')
print(f'jumlah information tourism: {len(tourism_info)}')

"""Setelah data csv di baca dan di letakkan pada variable diketahui bahwa data package tourism berjumlah 100 data rating berjumlah 10.000 jumlah user 300 dan jumlah tempat wisata 437

## Exploratory Data Unvariate

Setelah setiap file csv di deklarasikan ke dalam sebuah variable maka data per variable akan di analisis untuk mengetahui apa jenis data yang ada.

### Explorasi package tourism
"""

package_tourism

package_tourism.info()

"""Data package tourism berisi 7 kolom dengan 100 baris, variable kolom (Package, city, place_tourism1, place_tourism2, place_tourism3, place_tourism4) dengan package memiliki type data integer sedangan untuk 5 variable kolom lainnya bertype string/object

### Ekplorasi data *Tourism Ratings*
"""

tourism_rating

tourism_rating.info()

"""Pada data rating memiliki 3 kolom (User Id, place Id, place Ratting) dengan type data integer
* user id = untuk id user
* place id = untuk id dari tempat/lokasi wisata
* Place rating = untuk rating setiap lokasi yang diberikan user

lalu untuk mengetahui berapa nilai dari min-max rating maka diperlukan deskripsi statistika digunakan fungsi ```data.describe()```
"""

tourism_rating.describe().T

"""Diketahui nilai minimum rating adalah 1.0 dan maximum rating adalah 5.0 dengan jumlah data 10000



"""

tourism_rating.corr()

"""Plot data dengan rating sebagai x dan Place Id sebagai y"""

plt.bar(tourism_rating['Place_Ratings'], tourism_rating['Place_Id'])
plt.show

"""### Eksplorasi data User"""

tourism_user

tourism_user.info()

"""Diketahui data user berjumlah 300 dengan nama kolom user_id, location(asal user) dan age
Data akan ditampilkan dalam visualisasi, data akan di petakan berdasarkan user_Id dan asal provinsi dari user.

Dikarenakan data lokasi belum terpisah antara nama kota dan provinsi maka data lokasi akan dipecah menjadi kota dan provinsi.
"""

tourism_user[['kota', 'provinsi']] = tourism_user['Location'].str.split(',', expand=True)
tourism_user

"""Setelah data dipecah data akan divisualisasikan dengan menggunakan barplot User_Id sebagai height"""

plt.figure(figsize=(15, 10))
sns.barplot(tourism_user['provinsi'], tourism_user['User_Id'])
plt.show()

"""### Eksplorasi data Tourism information"""

tourism_info

tourism_info.info()

"""Di dalam file tourism_info terdapat 13 kolom tetapi, ada 2 kolom dengan nama yang tidak jelas/tidak bisa dipahami maka kolom tersebut akan dihapus."""

tourism_inform = tourism_info.loc[:, ~tourism_info.columns.str.contains('^Unnamed')]
tourism_inform

"""Diketahui bahwa :
- Place-Id : id untuk tempat wisata
- Place_Name: Nama dari lokasi wisata
- Description: penjelasan singkat mengenai tempat wisata tersebut
- Category: wisata termasuk dalam kategori apa(budaya, taman-hiburan dll)
- City: Lokasi tempat wisata(kota)
- Price: biaya masuk ke lokasi wisata 
- Rating : tingkat kepuasan pelanggan
- Time_Minutes: waktu perjalanan
- Coordinate: koordinat dari lokasi wisata
- Lat: Garis Lintang
- Long: Garis Bujur

Variable tourism_info memiliki 437 data, karena di dalam minut terdapat nan/missing value, makan missing value tersebut diganti dengan nilai rata-rata dari data pada column tersebut.

"""

tourism_inform.isna().sum()

"""Data time minutes memiliki banyak missing value, kalau data row yang mengandung missing value dihapus maka akan berakibat berkurangnya data hampir 50% sehingga untuk mengatasi itu, dipilih untuk drop data time_minus dari dataFrame, untuk mencegah kehilangan data yang banyak.

"""

tourism_inform = tourism_inform.drop(columns=["Time_Minutes"])
tourism_inform

"""## Content Based Filtering

### Data Processing

Data akan diproses untuk membuat sistem rekomendasi berdasarkan content.

Karena ingin membuat sistem content based filtering, data yang digunakan berasal data rating pengggunka yang akan digabung dengan data detail lokasi wisata. 

Gabungkan data tourism_rating dengan data tourism_inform
"""

rate_tourism = pd.merge(tourism_rating, tourism_inform,
                        on='Place_Id', how='left')
rate_tourism.head(2)

"""Data digabungkan berdasarakan kesamaan place ID data awal akan ditaruh di sebelah kiri dan data akhir berada di sebelah kanan.

Setelah data digabungkan cek di dalam penggabungan data tersebut, apakah terdaat missing value atau tidak.

"""

rate_tourism.isna().values.any()

"""Setelah di cek ternyata tidak ada data yang mengalami missing value

Dikarenakan data yang akan digunakan untuk content based filtering hanya 

data yang terdapat pada dataFrame rating_tourism dan tourism_inform untuk lokasi, nama tempat dan categori tempat wisata, maka data akan digabungkan sesuai dengan yang dibutuhkan berdasarakan place_idnya
"""

tourism_spot = pd.merge(tourism_rating, 
                        tourism_inform[['Place_Id', 'Place_Name',
                                        'City', 'Category']])
tourism_spot

"""Mengecek Nama kota apa saja yang terdapat dalam dataset kemudian mengurutkan dataset berdasarakan place_Id nya."""

tourism_spot.City.unique()

"""Data terdiri dari lima kota di Indonesia yaitu Yogyakarta, Semarang, Jakarta, Bandung dan Surabaya

data akan disimpan dalam variable data_preparation dan diurutkan berdasarakan Place_Id nya
"""

data_preparation = tourism_spot
data_preparation.sort_values('Place_Id')

"""Data yang digunakan adalah data yang bersifat unique sehingga data yang mengalami duplicate akan dihapus"""

#cek data duplicate 
data_preparation['Place_Id'].duplicated().sum()

#cek lokasi duplicate
data_preparation.loc[(data_preparation.duplicated())]

"""Diketahui bahwa data yang mengalami duplicate sebanyak 9563 maka data akan di drop/hapus berdasarkan nilai yang mengalami duplicated berdasarkan place_Id"""

data_preparation = data_preparation.drop_duplicates('Place_Id')
data_preparation

"""Data hanya tersisa 437 kolom

Kemudian data akan per column akan dirubah ke dalam per variable data dengan jenis data list.
Kemudian data akan dibuat per dictionary
"""

id = data_preparation['Place_Id'].tolist()
city = data_preparation['City'].tolist()
category = data_preparation['Category'].tolist()
name = data_preparation['Place_Name'].tolist()

# add to directory =
data_dict = pd.DataFrame({
    'place_id' : id,
    'place_name' : name,
    'place_city' : city,
    'place_category': category
})

data_dict.head()

"""### TFIDF (Matrix)

Untuk melakukan Content Filtering disini menggunakan TfidVectorizer yang berfungsi untuk konveri/merubah raw document menjadi matrix.

Data yang akan dirubah menjadi matrix adalah data City/nama Kota
"""

vector = TfidfVectorizer()
vector.fit(data_dict['place_city'])
vector.get_feature_names()

"""Data City memiliki 5 lokasi/kota yaitu :
* bandung
* Surabaya
* Jakarta 
* yogyakarta
* Semarang

`fit` digunakan untuk mempelajari kosakata dari data yang akan digunakan

`fit_transform` digunakan untuk mempelajari kosakata dan diubah menjadi IDF yang memberikan nilai berupa matrix

"""

matrix = vector.fit_transform(data_dict['place_city'])

matrix.shape

"""Data hasil konversi disimpan dalam variable matrix, data ini memiliki ukuran (437, 5), nilai 5 diambil dari 5 kota(feature yang berada pada place_city)

Data lalu akan dirubah menjadi satu dataFrame, didalam dataframe tersebut data hasil matrix akan dirubah menjadi vektor dengan menggunakan `.todense()` 

Data akan diambil 5 sample untuk sumbu x dan sumbu y
"""

pd.DataFrame(
    matrix.todense(),
    columns = vector.get_feature_names(),
    index = data_dict.place_name,
).sample(5, axis=0).sample(5, axis=1)

"""Diketahui bahwa seiap tempat akan berkorelasi dengan city/kota yang ditandai dengan nilai 1.0

Contoh Goa Gong memiliki nilai 1.0 pada kota Semarang, jadi Goa Gong terletak di semarang

### Cosine Similarity
Berfungsi untuk mengitung kesamaan data antar nama lokasi wisata.
Library yang digunakan adalah cosine_similarity



\begin{align}
  K(X, Y) = \frac{<X, Y>}{(||X|| * ||Y||}
\end{align}
"""

similarity = cosine_similarity(matrix)

similarity

"""Data hasil perhitungan kesaamaan atar fitur akan disimpan dalam variable dataframe. """

similarity_to_df = pd.DataFrame(
    similarity,
    columns = data_dict.place_name,
    index = data_dict.place_name,
)
print(f'Ukuran dari similarity : {similarity_to_df.shape}')
similarity_to_df.sample(5, axis=0).sample(5, axis=1)

"""Berdasarkan data diatas dikatahui bahwa data Pasar Beringharjo memiliki kesamaan dengan De Mata Museum Jogja, data diatas hanya diambil sample 5 saja

### Modeling

Membuat sistem rekomendasi berdasarkan kesamaan data yang diambil dari kesamaan kotanya
"""

def recomended_city_tourism(place_name,
                            similarity_name=similarity_to_df,
                            items=data_dict[['place_name', 'place_city']], 
                            n=20):
  i = similarity_name.loc[place_name].to_numpy().argpartition(
      range(-1, -n, -1)
  )
  h_index = similarity_name.columns[i[-1:-(n+1):-1]]
  h_index = h_index.drop(place_name, errors='ignore')
  return pd.DataFrame(h_index).merge(items).head(n)

"""Karena ingin meminta rekomendasi berdasarkan tempat Pulau Pramuka maka ditampilkan terlebih dahulu kriteria dari Pulau Pramuka"""

data_dict[data_dict.place_name.eq('Pulau Pramuka')]

"""Melakukan rekomendasi berdasarkan Pulau Pramuka menggunakan model yang telah di susun berdasarkan kesamaan lokasi/nama kota"""

recomended = recomended_city_tourism('Pulau Pramuka')
recomended

"""Karena data yang diseting untuk ditampilkan di dalam model adalah 20 data yang ditampilkan juga berjumlah 20 data tersebut memili nama kota/lokasi yang sama yaitu Jakarta

### Evaluasi Model

\begin{align}
  precission = \frac{recomendation relevant}{sum item recommended}
  \end{align}

Diketahui bahwa sistem merekomendasikan 20 tempat yang memiliki nilai berdasarkan kota yang sama, dari 20 item tersebut memiliki nilai kota yang sama yaitu jakarta
"""

precision = (len(recomended['place_city'] == 'Jakarta')/len(recomended))*100
print(f'precission: {precision} %')

"""Diketahui bahwa pecission dari data yang direkomendasikan adalah 100%, sehingga mengindikasikan sistem dapat berjalan dengan baik dikarenakan yang digunakan untuk matrix nya adalah nama kota (hanya ada 5 kategori kota)

## Colaborative Filtering

### Data Understanding

Diketahui bahwa data rating memiliki 10000 row dengan 3 kolom
"""

colaborative_filtering = tourism_rating

colaborative_filtering

"""### Data Preparation
Data yang akan digunakan yaitu data user_id dan data place_id data tersebut diambil yang unique kemudian diajadikan list. 

Selanjutnya dilakukan encode data id tersebut ke dalam uturan angka integer, dikarenakan data user dan place id adalah data interger maka data tersebut dirubah terlebih dahulu menjadi data string 

"""

colaborative_filtering['User_Id'] = colaborative_filtering['User_Id'].astype(np.object)
colaborative_filtering['Place_Id'] = colaborative_filtering['Place_Id'].astype(np.object)

colaborative_filtering.head()

colaborative_filtering.info()

"""Lalu data user dan place dirubah menjadi list untuk selanjutnya dilakukan encode data tersebut."""

id_user = colaborative_filtering['User_Id'].unique().tolist()
id_place = colaborative_filtering['Place_Id'].unique().tolist()


user_encode = {user: u for u, user in (enumerate(id_user))}
#encode angka ke user
encode_user = {u: user for u, user in (enumerate(id_user))}


place_encode = {place: p for p, place in enumerate(id_place)}
#encioding dari angka ke place
encode_place = {p : place for p, place in enumerate(id_place)}

print(user_encode)
print(encode_user)
print(place_encode)
print(encode_place)

"""data hasil encode akan diambil jumlahnya dengan menggunakan `len`

Dilanjutkan dengan mencari nilai min dan max yang berfungsi untuk min max scaler dari place rating (place rating sebagai nilai label yang akan digunakan ketika training data)
"""

num_u = len(user_encode)
num_p = len(place_encode)

colaborative_filtering['Place_Ratings'] = colaborative_filtering['Place_Ratings'].values.astype(np.float64)

min_r = min(colaborative_filtering['Place_Ratings'])
max_r = max(colaborative_filtering['Place_Ratings'])

print(min_r)
print(max_r)

"""Nilai max nya adalah 5 dan min 1

Data User_Id dan Place_Id yang telah di encode akan ditambahkan di dalam data colaborative filtering(dataframe ratings)
"""

#memasukkan hasil encode ke dalam dataFrame
colaborative_filtering['id_user'] =colaborative_filtering['User_Id'].map(user_encode)
colaborative_filtering['id_place'] =colaborative_filtering['Place_Id'].map(place_encode)

colaborative_filtering.head(3)

colaborative_filtering.info()

"""### Split data Train and Val

data akan dipecah menjadi train 80% dan val 20% dengan data yang akan menjadi label dilakukan minmazScaler/normalisasi
"""

user_place = colaborative_filtering[['id_user', 'id_place']].values

rating = colaborative_filtering['Place_Ratings'].apply(
    lambda u: (u - min_r )/(max_r-min_r)
).values

train_user, val_user, train_rating, val_rating = train_test_split(user_place, rating,
                                                                  test_size=0.2)

print(f'jumlah dari data train {len(train_user)}')
print(f'Jumlah dari data validation {len(val_user)}')

"""Data train berjumlah 7000 dan data val berjumlah 3000, setelah proses split data

### Training Model

Setelah data berhasil di periapkan dilanjutkan dengan membuat model baru dengan memanfaakan tensorflow berdasarkan proses embeding dari user_id dan place_id, dengan menggunakan perkalian dot product [0, 1] menggunakan sigmoid sebagai aktivasi
"""

class filtering_based_ratings(tf.keras.Model):
  def __init__(self, num_u, num_p, size_e, **place):
    super(filtering_based_ratings, self).__init__(**place)
    self.num_u = num_u
    self.num_p = num_p
    self.size_e = size_e
    self.user_e = Embedding(
        num_u,
        size_e,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = tf.keras.regularizers.l1(1e-6)
    )
    self.bias1 = Embedding(num_u, 1)
    self.place_e = Embedding(
        num_p,
        size_e,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = tf.keras.regularizers.l1(1e-6)
      )
    self.bias2 = Embedding(num_p, 1)

  def call(self, ins):
    u = self.user_e(ins[:, 0])
    bias1 = self.bias1(ins[:, 0])
    p = self.place_e(ins[:, 1])
    bias2 = self.bias2(ins[:, 1])

    user = tf.tensordot(u, p, 2)
    merge = user + bias1 + bias2

    return tf.nn.sigmoid(merge)

"""setelah model terbentuk data akan di training menggunkan optimizer ada dengan los binary_crossentropy berdasarakan metrics MAE(Mean Absolute Error)"""

model = filtering_based_ratings(num_u, num_p, 128)
model.compile(optimizer = Adam(learning_rate=1e-4),
              loss='binary_crossentropy',
              metrics=[tf.keras.metrics.MeanAbsoluteError()])

"""Data di training dengan 10 epoch dan batch_size nya 5 

\begin{align}
  \frac{jumlah data train}{batchsize} = \frac{7000}{5} = 1400/perepochs
\end{align}
"""

h = model.fit(
    x=train_user, 
    y=train_rating, 
    batch_size=5,
    validation_data=(val_user, val_rating), 
    epochs=50, verbose=2)

"""Didapatkan nilai mae terendah adalah 0.2514 (25%) yang berarti nilai mae masih terlalu tinggi sehingga bisa berakibat data yang akan direkomendasikan kurang sesuai dengan nilai loss yang masih tinggi

#### Ploting Model
"""

train = h.history['mean_absolute_error']
val = h.history['val_mean_absolute_error']

e = range(len(train))

plt.plot(e, train, label='training')
plt.plot(e, val, label="validations")
plt.legend()
plt.show()

"""### Evaluasi Model

Evaluasi dilakukan untuk mengetahui bagaimana tingkat error data sehingga bisa mengetau bagaimana performa dari data. Evaluasi menggunakan metrics MSE. 

\begin{align}
        \text{MAE } = (&\sum_{i=0}^n \mid y_i -y_i\mid) /n
    \end{align}
"""

ypred = model.predict(val_user)
mae = mean_absolute_error(val_rating, ypred)
print(mae)

"""nilai mae yang di dapatkan tidak terlalu tinggi sehingga data ini cocok untuk digunakan dalam sistem rekomendasi, tetapi mae dengan nilai 31% terbilang cukup tinggi sehingga bisa mengakibatkan overfitting data

### System Rekomendation from user

data yang digunakan untuk proses ini diambil dari data yang sudah disimpan dalam directory pada proses data processing pada teknik content based learning
"""

place = data_dict

rate = pd.read_csv('/content/destination_tourism/tourism_rating.csv')
place.head(3)

"""Data akan diambil sampel 1 data yang digunakan diambil dari data nomer 2, lalu data tersebut nantinya id nya akan dibandingkan id user sehingga bisa meghasikan rekomendasi tempat wisata,

Data tersebut nantinya akan dibagi menjadi lokasi yang pernah dikunjungi dan lokasi yang tidak pernah dikunjungi sebelumnya.
"""

#user = input("Masukkan user id ")
user = rate.User_Id.sample(2).iloc[0]
print(user)
place_visited = rate[rate.User_Id == user]

place_not = place[~place['place_id'].isin(place_visited.Place_Id.values)]['place_id']
place_not = list(
    set(place_not)
    .intersection(set(encode_place.keys()))
)
place_not = [[place_encode.get(a)] for a in place_not]
user_recomm = user_encode.get(user)
user_array = np.hstack(([[user_recomm]] * len(place_not), place_not))

"""Dilihat bahwa id yang diambil adalah 284 id tersebut melakukan penilaian ke beberapa tempat dengan nilai rating yang berbeda-beda"""

place_visited

predict = model.predict(user_array).flatten()

top_rate = predict.argsort()[-5:][::-1]
recomended_place = [
                    place_encode.get(place_not[a][0]) for a in top_rate
]

print(f' Rekomendasi dari user ber id : {user}')

print('---' * 10)
print("rekomendasi user dengan nilai rating tertinggi ")

top_place = (
    place_visited.sort_values(by= 'Place_Ratings')
).head(1).Place_Id.values

top_row = place[place['place_id'].isin(top_place)]
for r in top_row.itertuples():
  print(r.place_name, ':', r.place_city)

print('----' * 10)

recomended_top_rating = place[place['place_id'].isin(recomended_place)]
for r in recomended_top_rating.itertuples():
  print(r.place_name, ':', r.place_city)

"""karena sistem rekomendasi yang sudah di setting dengan hanya menampilkan 5 data saja maka data rekomendasi hanya 5 data diambil berdasarkan rating tertinggi dari user terhadap tempat-tempat wisata tersebut"""