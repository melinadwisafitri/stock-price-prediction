# -*- coding: utf-8 -*-
"""bismillah.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A9rtqBW9-vxk8FnyN3MhQQq6f9fQu6b7

#Sistem Rekomendasi mencari lokasi wisata di Indonesia

Sistem digunakan untuk meberikan rekomendasi nama destinasi lokasi wisata berdasarkan cotent bases filtering (yang digunakan adalah nama lokasi yang di filter berdasarkan kesamaan lokasi/kota)

## Data Source
Data diambil dari kaggle, terdapat 4 file di dalam file zip

[Source data Indonesia Tourism Destination](https://www.kaggle.com/aprabowo/indonesia-tourism-destination)

Import semua Library yang akan digunakan, import dilakukan pada proses awal karena untuk mempermudah proses pengerjaan.
"""

!pip install kaggle

import numpy as np
import pandas as pd
import seaborn as sns
import tensorflow as tf
import matplotlib.pyplot as plt

from pathlib import Path
from google.colab import files
from tensorflow.keras import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Embedding
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer

"""Karena Data diambil dari kaggle, untuk mempermudah proses load data bisa menggunakan library kaggle.

Install kaggle, lalu upload file key token yang telah di *generate*  dari account yang dimiliki. Lalu membuat direktory baru bernama kaggle, pindah data kaggle.json yang sudah kita upload, arahkan ke folder kaggle yang sudah dibuat sebelumnya, lalu atur *permission access* supaya owner/user, grup, other bisa untuk *read, write and execution* terhadap data kaggle.json dengan menggunakan ```chmood 600```
"""

upload = files.upload()
for k in upload.keys():
  print(k)

!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json

"""## Data Loading

Download dataset Indonesia torism destination dengan menggunakan API kaggle file akan disimpan dalam ```/content/```
"""

!kaggle datasets download -d aprabowo/indonesia-tourism-destination

"""### Unzip File zip

Setelah data di download, ternyata data berbentuk file zip, yang didalamnya terdapat 4 file csv yaitu:
1. tourism_ with_id.csv : file ini berisi informasi lokasi wisata dari 5 kota di Indonesia 
2. user.csv : file ini berisi mengenai data pengguna (data dummy)
3. tourism_rating.csv : file ini berisi mengenai data rating dari bebrapa tempat lokasi wissata yang diberikan oleh user 
4. package_tourism.csv : file ini berisi package lokasi wisata

Buat satu directory yang berfungsi untuk menampung data dari indonesia-tourism-destination.zip dengan menggunakan 

```!mkdir name_directory```

untuk ektract data zip bisa menggunakan script 



```
!unzip -q indonesia-tourism-destination.zip
```
kenapa terdapat -q didalam script ```!unzip``` dikarenakan untuk menyembunyikan comment yang muncul
"""

!mkdir -p /content/destination_tourism
!unzip -q indonesia-tourism-destination.zip -d /content/destination_tourism

"""### Load Data
Load data masing-masing file kedalam variable yang mengambarkan nama dari file tersebut, data di load dengan menggunakan library **Pandas**

Kemudian cek berapa jumlah masing-masing data dalam varible tersebut dengan menggunkan fungsi ```len()```
"""

package_tourism = pd.read_csv('/content/destination_tourism/package_tourism.csv')
tourism_rating = pd.read_csv('/content/destination_tourism/tourism_rating.csv')
tourism_user = pd.read_csv('/content/destination_tourism/user.csv')
tourism_info = pd.read_csv('/content/destination_tourism/tourism_with_id.csv')

print(f'Jumlah data package tourism : {len(package_tourism)}')
print(f'Jumlah data tourism rating: {len(tourism_rating)}')
print(f'Jumlah user: {len(tourism_user)}')
print(f'jumlah information tourism: {len(tourism_info)}')

"""Setelah data csv di baca dan di letakkan pada variable diketahui bahwa data package tourism berjumlah 100 data rating berjumlah 10.000 jumlah user 300 dan jumlah tempat wisata 437

## Exploratory Data Unvariate

Setelah setiap file csv di deklarasikan ke dalam sebuah variable maka data per variable akan di analisis untuk mengetahui apa jenis data yang ada.

### Explorasi package tourism
"""

package_tourism

package_tourism.info()

"""Data package tourism berisi 7 kolom dengan 100 baris, variable kolom (Package, city, place_tourism1, place_tourism2, place_tourism3, place_tourism4) dengan package memiliki type data integer sedangan untuk 5 variable kolom lainnya bertype string/object

### Ekplorasi data *Tourism Ratings*
"""

tourism_rating

tourism_rating.info()

"""Pada data rating memiliki 3 kolom (User Id, place Id, place Ratting) dengan type data integer
* user id = untuk id user
* place id = untuk id dari tempat/lokasi wisata
* Place rating = untuk rating setiap lokasi yang diberikan user

lalu untuk mengetahui berapa nilai dari min-max rating maka diperlukan deskripsi statistika digunakan fungsi ```data.describe()```
"""

tourism_rating.describe().T

"""Diketahui nilai minimum rating adalah 1.0 dan maximum rating adalah 5.0 dengan jumlah data 10000



"""

tourism_rating.corr()

"""Plot data dengan rating sebagai x dan Place Id sebagai y"""

plt.bar(tourism_rating['Place_Ratings'], tourism_rating['Place_Id'])
plt.show

"""### Eksplorasi data User"""

tourism_user

tourism_user.info()

"""Diketahui data user berjumlah 300 dengan nama kolom user_id, location(asal user) dan age
Data akan ditampilkan dalam visualisasi, data akan di petakan berdasarkan user_Id dan asal provinsi dari user.

Dikarenakan data lokasi belum terpisah antara nama kota dan provinsi maka data lokasi akan dipecah menjadi kota dan provinsi.
"""

tourism_user[['kota', 'provinsi']] = tourism_user['Location'].str.split(',', expand=True)
tourism_user

"""Setelah data dipecah data akan divisualisasikan dengan menggunakan barplot User_Id sebagai height"""

plt.figure(figsize=(15, 10))
sns.barplot(tourism_user['provinsi'], tourism_user['User_Id'])
plt.show()

"""### Eksplorasi data Tourism information"""

tourism_info

tourism_info.info()

"""Di dalam file tourism_info terdapat 13 kolom tetapi, ada 2 kolom dengan nama yang tidak jelas/tidak bisa dipahami maka kolom tersebut akan dihapus."""

tourism_inform = tourism_info.loc[:, ~tourism_info.columns.str.contains('^Unnamed')]
tourism_inform

"""Diketahui bahwa :
- Place-Id : id untuk tempat wisata
- Place_Name: Nama dari lokasi wisata
- Description: penjelasan singkat mengenai tempat wisata tersebut
- Category: wisata termasuk dalam kategori apa(budaya, taman-hiburan dll)
- City: Lokasi tempat wisata(kota)
- Price: biaya masuk ke lokasi wisata 
- Rating : tingkat kepuasan pelanggan
- Time_Minutes: waktu perjalanan
- Coordinate: koordinat dari lokasi wisata
- Lat: Garis Lintang
- Long: Garis Bujur

Variable tourism_info memiliki 437 data, karena di dalam minut terdapat nan/missing value, makan missing value tersebut diganti dengan nilai rata-rata dari data pada column tersebut.

"""

tourism_inform.isna().sum()

"""Data time minutes memiliki banyak missing value, kalau data row yang mengandung missing value dihapus maka akan berakibat berkurangnya data hampir 50% sehingga untuk mengatasi itu, dipilih untuk drop data time_minus dari dataFrame, untuk mencegah kehilangan data yang banyak.

"""

tourism_inform = tourism_inform.drop(columns=["Time_Minutes"])
tourism_inform

"""## Content Based Filtering

Teknik yang digunakan untuk memberikan rekomendasi kepada user berdasarkan history pencarian yang mirip.

### Data Preparation

Menyiapkan data yang akan digunakan untuk membuat sistem rekomendasi berdasarkan history item.

Data yang digunakan adalah data tourism_with_id/tourism_inform data yang berisi mengenai penjelasan lokasi wisata, data berjumlah 437. Data yang digunaka akan diambil dan disimpan kedalam sebuah dataFrame baru, data yang akan diambil adalah place_id, place_name dan place_city.
"""

data_preparation = tourism_inform

id_place = data_preparation['Place_Id'].tolist()
city = data_preparation['City'].tolist()
name_place = data_preparation['Place_Name'].tolist()
#category_place = data_preparation['Category']
data_content = pd.DataFrame({
    'place_id' :id_place,
    'place_city' : city,
    'place_name': name_place,
    #'place_category': category_place,
})
data_content

data_content.isna().values.any()

"""### TFIDF (Matrix)

Untuk melakukan Content Filtering disini menggunakan TfidVectorizer yang berfungsi untuk konvert/merubah raw document menjadi matrix.

Data yang akan dirubah menjadi matrix adalah data City/nama Kota
"""

vector = TfidfVectorizer()

vector.fit(data_content['place_city'])
vector.get_feature_names()

"""Data City memiliki 5 lokasi/kota yaitu :
* bandung
* Surabaya
* Jakarta 
* yogyakarta
* Semarang

`fit` digunakan untuk mempelajari kosakata dari data yang akan digunakan

`fit_transform` digunakan untuk mempelajari kosakata dan diubah menjadi IDF yang memberikan nilai berupa matrix

"""

matrix = vector.fit_transform(data_content['place_city'])
matrix.shape

"""Data hasil konversi disimpan dalam variable matrix, data ini memiliki ukuran (437, 5), nilai 5 diambil dari 5 kota(feature yang berada pada place_city)

Data lalu akan dirubah menjadi satu dataFrame, didalam dataframe tersebut data hasil matrix akan dirubah menjadi vektor dengan menggunakan `.todense()` 

Data akan diambil 5 sample untuk sumbu x dan sumbu y
"""

pd.DataFrame(
    matrix.todense(),
    columns = vector.get_feature_names(),
    index = data_content.place_name,
).sample(5, axis=0).sample(5, axis=1)

"""Diketahui bahwa seiap tempat akan berkorelasi dengan city/kota yang ditandai dengan nilai 1.0

Contoh Goa Gong memiliki nilai 1.0 pada kota Semarang, jadi Goa Gong terletak di semarang

### Cosine Similarity
Berfungsi untuk mengitung kesamaan data antar nama lokasi wisata.
Library yang digunakan adalah cosine_similarity



\begin{align}
  K(X, Y) = \frac{<X, Y>}{(||X|| * ||Y||}
\end{align}
"""

similarity = cosine_similarity(matrix, matrix)

similarity

"""Data hasil perhitungan kesaamaan atar fitur akan disimpan dalam variable dataframe. """

similarity_to_df = pd.DataFrame(
    similarity,
    columns = data_content.place_name,
    index = data_content.place_name,
)
print(f'Ukuran dari similarity : {similarity_to_df.shape}')
similarity_to_df.sample(5, axis=0).sample(5, axis=1)

"""Berdasarkan data diatas dikatahui bahwa Taman Cattleyamemiliki kesamaan dengan Pasar Seni data diatas iambil sample 5 saja

### Modeling

Membuat sistem rekomendasi berdasarkan kesamaan data yang diambil dari kesamaan kotanya
"""

def recomended_city_tourism(place_name,
                            similarity_name=(similarity_to_df),
                            items=data_content[['place_name', 'place_city']], 
                            n=10):
  i = similarity_name.loc[place_name].to_numpy().argpartition(
      range(-1, -n, -1)
  )
  h_index = similarity_name.columns[i[-1:-(n+2):-1]]
  h_index = h_index.drop(place_name, errors='ignore')
  return pd.DataFrame(h_index).merge(items).head(n)

"""Karena ingin meminta rekomendasi berdasarkan tempat Pulau Pramuka maka ditampilkan terlebih dahulu kriteria dari Pantai Ancol"""

data_content[data_content.place_name.eq('Pantai Ancol')]

"""Melakukan rekomendasi berdasarkan Pantai Ancol menggunakan model yang telah di susun berdasarkan kesamaan lokasi/nama kota"""

recomended = recomended_city_tourism('Pantai Ancol')
recomended

"""Karena data yang diseting untuk ditampilkan di dalam model adalah 10 data yang ditampilkan juga berjumlah 10 data tersebut memililki nama kota/lokasi yang sama yaitu Jakarta

### Evaluasi Model

\begin{align}
  precission = \frac{recomendation relevant}{sum item recommended}
  \end{align}

Diketahui bahwa sistem merekomendasikan 10 tempat yang memiliki nilai berdasarkan kota yang sama, dari 10 item tersebut memiliki nilai kota yang sama yaitu jakarta
"""

precision = (len(recomended['place_city'] == 'Jakarta')/len(recomended))*100
print(f'precission: {precision} %')

"""Diketahui bahwa pecission dari data yang direkomendasikan adalah 100%, sehingga mengindikasikan sistem dapat berjalan dengan baik dikarenakan yang digunakan untuk matrix nya adalah nama kota (hanya ada 5 kategori kota)

## Colaborative Filtering

Tenik ini digunakan untuk merekomendasikan user berdasarkan user_group atau bebeberapa user lain. Data yang digunakan adalah data rating dan data_inform yang sudah diberikan oleh pengguna/user.

### Data Understanding

Untuk membuat colaborative Filtering data yang digunakan adalah data rating
"""

rate = tourism_rating
rate

"""### Data Preparation

Menggabungkan data rating dengan data informasi tempat bertujuan untuk mengetahui secara detail dari informasi berdasarkan rating yang diberikan user.
Data yang akan digunakan untuk Colaborative FIltering yaitu data yang berada di file rating sebelumnya dan data tempat serta rating tempat secara keseluruhan yang diambil dari data_information.
"""

data_colaborative = pd.merge(rate, tourism_inform[[
                                                  'Place_Id',
                                                  'Place_Name', 
                                                  'City',
                                                  'Category',
                                                  'Rating'
]])
data_colaborative

"""Data berjumlah 1000 baris, lalu cek data apakah data memiliki missing value atau tidak"""

data_colaborative.isna().values.any()

"""Data tidak ada yang mengalami missing values.

Kemudian untuk mempermudah, dilakukan inisialisasi variable baru terhadap data yang sudah diambil terlebih dahulu. Dan data diurutkan berdasarkan Id tempat
"""

data_preparation_fix = data_colaborative.sort_values('Place_Id')
data_preparation_fix

"""Data yang akan digunakan yaitu data user_id dan data place_id data tersebut diambil yang unique kemudian diajadikan list. 

Selanjutnya dilakukan encode data id_place dan id_user, berfungsi untuk melakukan indexing/membuat data menjadi integer.

Data user dan place dirubah menjadi list untuk selanjutnya dilakukan encode data tersebut.
"""

id_user = data_preparation_fix['User_Id'].unique().tolist()
id_place = data_preparation_fix['Place_Id'].unique().tolist()


user_encode = {user: u for u, user in (enumerate(id_user))}
#encode angka ke user
encode_user = {u: user for u, user in (enumerate(id_user))}


place_encode = {place: p for p, place in enumerate(id_place)}
#encioding dari angka ke place
encode_place = {p : place for p, place in enumerate(id_place)}

print(user_encode)
print(encode_user)
print(place_encode)
print(encode_place)

"""Data User_Id dan Place_Id yang telah di encode akan ditambahkan di dalam dataframe colaborative filtering_fix"""

#memasukkan hasil encode ke dalam dataFrame
data_preparation_fix['id_user'] = data_preparation_fix['User_Id'].map(user_encode)
data_preparation_fix['id_place'] = data_preparation_fix['Place_Id'].map(place_encode)

data_preparation_fix.head()

"""Data yang akan displit, sebelumnya diinisialisasi dahulu, dipecah kedalam masing-masing vaiable, jumlah data place dan user encode juga akan disimpan dalam varible sendiri-sendiri, jumlah data ini yang akan digunakan untuk menjalakan model."""

data = data_preparation_fix[['id_user', 'id_place']].values
rating_place_from_user =data_preparation_fix['Place_Ratings'].values

num_p = len(place_encode)
num_u = len(user_encode)
print(f'jumlah data user {num_u}\njumlah data tempat {num_p}')

"""### Split data Train and Val

MinMaxScaler data Ratings, Data rating di lakukan scalling menggunakan MinMaxScaler dari library sklearn, fungsi scaler ini untuk menormalisasi data, sehingga data bisa lebih optimal untuk digunakan.
"""

scall = MinMaxScaler(feature_range=(0, 1))

"""Dikarenakan data rating yang sudah diambil tidak bisa dingunakan kerena ukuran dari dari maka data akan reshape dulu dengan (-1, 1) yang memiliki arti menambah satu kolom. """

rating_place_from_user_reshape = rating_place_from_user.reshape(-1, 1)
print(rating_place_from_user.shape)

rating_place_from_user_scall = scall.fit_transform(rating_place_from_user_reshape)
rating_place_from_user_scall.shape

"""Data akan dipecah menjadi train 80% dan val 20%, split data memnfaatkan fitur train_test_split dari sklearn, untuk mempercepat process."""

train_user, val_user, train_rating, val_rating = train_test_split(
    data, rating_place_from_user_scall, test_size=0.2
)

print(f'jumlah dari data train {len(train_user)}')
print(f'Jumlah dari data validation {len(val_user)}')

"""### Training Model

Setelah data berhasil di periapkan dilanjutkan dengan membuat model baru dengan memanfaakan tensorflow berdasarkan proses embeding dari user_id dan place_id, dengan menggunakan perkalian dot product [0, 1] menggunakan sigmoid sebagai aktivasi
"""

class filtering_based_ratings(tf.keras.Model):
  def __init__(self, num_u, num_p, size_e, **place):
    super(filtering_based_ratings, self).__init__(**place)
    self.num_u = num_u
    self.num_p = num_p
    self.size_e = size_e
    self.user_e = Embedding(
        num_u,
        size_e,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = tf.keras.regularizers.l1(1e-6)
    )
    self.bias1 = Embedding(num_u, 1)
    self.place_e = Embedding(
        num_p,
        size_e,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = tf.keras.regularizers.l1(1e-6)
      )
    self.bias2 = Embedding(num_p, 1)

  def call(self, ins):
    u = self.user_e(ins[:, 0])
    bias1 = self.bias1(ins[:, 0])
    p = self.place_e(ins[:, 1])
    bias2 = self.bias2(ins[:, 1])

    user = tf.tensordot(u, p, 2)
    merge = user + bias1 + bias2

    return tf.nn.sigmoid(merge)

"""setelah model terbentuk data akan di training menggunkan optimizer ada dengan los binary_crossentropy berdasarakan metrics MAE(Mean Absolute Error)"""

model = filtering_based_ratings(num_u, num_p, 16)
model.compile(optimizer = Adam(learning_rate=1e-3),
              loss='binary_crossentropy',
              metrics=[tf.keras.metrics.MeanAbsoluteError()])

"""Data di training dengan 10 epoch dan batch_size nya 5 

\begin{align}
  \frac{jumlah data train}{batchsize} = \frac{7000}{5} = 1400/perepochs
\end{align}
"""

h = model.fit(
    x=train_user, 
    y= train_rating, 
    batch_size=5,
    validation_data=(val_user, val_rating), 
    epochs=50, verbose=2)

"""Didapatkan nilai mae terendah adalah 0.2465 (25%) yang berarti nilai mae masih terlalu tinggi sehingga bisa berakibat data yang akan direkomendasikan kurang sesuai dengan nilai loss yang masih tinggi

#### Ploting Model
"""

train = h.history['mean_absolute_error']
val = h.history['val_mean_absolute_error']

e = range(len(train))

plt.figure(figsize=(15, 10))
plt.plot(e, train, label='training')
plt.plot(e, val, label="validations")
plt.legend()
plt.show()

"""### Evaluasi Model

Evaluasi dilakukan untuk mengetahui bagaimana tingkat error data sehingga bisa mengetau bagaimana performa dari data. Evaluasi menggunakan metrics MSE. 

\begin{align}
        \text{MAE } = (&\sum_{i=0}^n \mid y_i -y_i\mid) /n
    \end{align}
"""

ypred = model.predict(val_user)
mae = mean_absolute_error(val_rating, ypred)
print(mae)

"""nilai mae yang di dapatkan tidak terlalu tinggi sehingga data ini cocok untuk digunakan dalam sistem rekomendasi, tetapi mae dengan nilai 34% terbilang cukup tinggi sehingga bisa mengakibatkan overfitting data

### System Rekomendation from user

data yang digunakan untuk proses ini diambil dari data yang sudah disimpan dalam directory pada proses data processing pada teknik content based learning
"""

place = tourism_inform

ratings = data_colaborative
place

"""Data akan diambil sampel 1 data yang digunakan diambil dari data nomer 2, lalu data tersebut nantinya id nya akan dibandingkan id user sehingga bisa meghasikan rekomendasi tempat wisata,

Data tersebut nantinya akan dibagi menjadi lokasi yang pernah dikunjungi dan lokasi yang tidak pernah dikunjungi sebelumnya.
"""

#user = input("Masukkan user id ")
user = ratings.User_Id.sample(2).iloc[0]
print(user)
place_visited = ratings[ratings.User_Id == user]

place_not = place[~place['Place_Id'].isin(place_visited.Place_Id.values)]['Place_Id']
place_not = list(
    set(place_not)
    .intersection(set(encode_place.keys()))
)
place_not = [[place_encode.get(a)] for a in place_not]
user_recomm = user_encode.get(user)
user_array = np.hstack(([[user_recomm]] * len(place_not), place_not))

place_visited

"""Dilihat bahwa id yang diambil adalah 225 id tersebut melakukan penilaian ke beberapa tempat dengan nilai rating yang berbeda-beda"""

predict = model.predict(user_array).flatten()

top_rate = predict.argsort()[-5:][::-1]
recomended_place = [
                    place_encode.get(place_not[a][0]) for a in top_rate
]

print(f' Rekomendasi dari user ber id : {user}')

print('---' * 10)
print("rekomendasi user dengan nilai rating tertinggi ")

top_place = (
    place_visited.sort_values(by='Place_Ratings', ascending=False)
).head(3).Place_Id.values

top_row = place[place['Place_Id'].isin(top_place)]
for r in top_row.itertuples():
  print(r.Place_Name, ':', r.Category)

print('----' * 10)
print("5 rekomendasi tertinggi")

recomended_top_rating = place[place['Place_Id'].isin(recomended_place)]
for r in recomended_top_rating.itertuples():
  print(r.Place_Name, ':', r.Category)

"""karena sistem rekomendasi yang sudah di setting dengan hanya menampilkan 5 data saja maka data rekomendasi hanya 5 data diambil berdasarkan rating tertinggi dari user terhadap tempat-tempat wisata tersebut"""